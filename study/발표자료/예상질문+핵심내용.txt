코사인 유사도: 코사인 계산으로 두 벡터의 방향이 얼마나 유사한지, 벡터의 크기는 무시
코사인 거리: (수식= 1-코사인유사도), 벡터간 차이를 거리로 변환하여 표현, 거리측정
유클리드 거리: 벡터의 크기와 방향을 모두 고려

Q.통계기반기법에서는 미니배치 학습이 안되는지 
동시발생 행렬은 전체 데이터셋에서 빈도를 측정하는데, 부분데이터로 학습하면 일관되지 않거나 정확하지 않을 수 있음

Q.직교 행렬 
코사인 유사도 0의 의미는 서로 완전 독립적인 벡터

Q.truncated svd와 svd차이 설명
두개다 행렬 분해 기법이지만, Truncated SVD는 SVD의 변형으로, 지정한 몇개의 특잇값만 보존하여 계산량과 차원을 효율적으로 줄이는 방법

Q.CBOW 순서를 고려하지 않는다는게 무슨 의미?
윈도우 안에서는 순서를 고려하지 않음 
CBOW 모델 그림을 보면 입력층 순서가 바뀌어도 
결국 더해지기 때문에 무관

Q.the cat is cute를 CBOW에 넣어 cat을 윈도우크기 1로 
추출 시 입력층으로 보는 것? 
the 와 is  :원핫표현 
the       1
cat       0
is         0 
cute     0 이런식 

Q.softmax와 sigmoid 차이 
다중분류와 이진분류 
결과 차이 softmax는 1을 3개의 확률로 (총합이 1이 되게)
sigmoid는 그래프 생각하면 하나의 확률을 추출?

Q. 부정적 예가 무엇인지, 
부정적 예 전부를 쓴다는게 무슨 의미?

Q.네거티브 샘플링은 주로 CBOW와 
skip gram중 어디에 사용하는지 
 둘 다 사용 

Q.Truncated BPTT 다시 설명 ,단점은 없는지 
순전파에서 나온 은닉 상태 h9는 메모리에 저장되어 
BPTT보다 더 많은 메모리가 소모된다는 단점  

Q.BPTT보다 Truncated BPTT가 좋은 이유 
둘의 과정 설명 순전파,역전파 
중간에 끊어줌으로써 기울기 소실 문제 해결 

Q. 은닉층 뉴런 수를 더 적게 하면 좋은 점 
과적합을 막아준다 
차원수를 줄여 계산 비용을 절감하고 학습 속도를 빠르게 한다

Q. 단어의 분산 표현이라는 것이 정확히 무엇인지, 무엇이 분산되어 있는 것인지
A: 분포가설에 기초한 벡터화이기에 분포 가설을 이용하여 단어를 학습하고 단어의 의미를 여러 차원에 분산하여 표현한 것임

Q. 맥락을 볼 때 윈도우 사이즈가 커질수록 어떻게 되는지
A: 윈도우 사이즈는 하이퍼파라미터이나, 너무 늘린다면 병목현상이 발생할 수도 있음

Q. 코사인 유사도와 유클리드 거리의 차이점이 무엇인지 
A: 코사인 유사도는 각도 기반이기에 문장 길이에 대한 영향을 덜 받을 수 있는 방법임

Q. PMI에서 두 단어의 동시 발생 횟수가 0이면  log₂ = -∞ 가 나오는데, 이게 왜 문제가 되는지
A: PMI식 연산을 할 때 분모에 0이 있으면 정의 자체가 되지 않음
분산 표현 시 -∞이면 비교 자체가 되지 않으니 0으로 만드는 게(PPMI)가 아닌지
코드 메모리 측면에서 피하는 것이 아닌지 (여러 의견이 있었음)

Q. SVD에서 2차원으로 근사하는데 실제 몇 차원으로 근사하는 건지 차원 수를 정하는 것이 궁금
A: 차원 수를 정하는 것은 하이퍼파라미터로 알고 있음. 그러나 차원을 너무 작게 한다면 본래 의미가 유지되지 못할 수도 있고
너무 크게 한다면 차원 축소의 의미가 사라질 수도 있기에 적절한 크기로 설정하는 것이 필요할 것 같음

Q. SVD 특잇값 분해에서 실제 차원이 축소된 범위는 어디인지
A: 실제 차원이 축소된 범위는 U임 대각 행렬(S)의 원소 중 몇 개만을 추출하고 여기에 대응하는 U와 V의 원소도 함께
깎아내려 차원을 줄이는 것. S의 대각 성분은 특이값이 큰 순서대로 나열되어 있어 중요한 정보가 앞 열에 몰려있기에
잘라내어도 의미가 거의 유지된 채 공간을 적게 차지하게 됨

Q. CBOW의 입력층에서 은닉층으로 갈 때의 연산은 어떻게 되는지
A: 입력값(맥락의 원핫벡터)와 Win의 행렬곱 후 평균을 냄

Q. CBOW는 은닉층이 한 개만 존재하는데 여러 개 두면 안되는지
A: CBOW는 입력측 가중치 행렬을 통해 단어의 분산 표현을 얻는 데 중점을 두고 있어 차이가 있다 생각
오토인코더는 특징과 같은 잠재변수를 얻는 데에 목적을 두고 있어 은닉층 여러 개를 둔 것

Q. CBOW에서 순서를 고려할 수 있는 방법이 있는지
A: 입력마다 은닉층을 두면 순서를 고려할 수 있으나, RNN을 사용하는 것이 더욱 효율적

Q. CBOW 모델에서 맥락 추출 기준이나 방법이 있는지
A: 위에서 말했듯, 하이퍼파라미터

Q. skip-gram과 CBOW 각각의 장단점
A: 단어 분산 표현의 정밀도 면에서 skip-gram의 결과가 더 좋은 경우가 많고, 말뭉치가 커질수록
저빈도 단어나 유추문제의 성능 면에서 더 뛰어난 경향이 있음
학습 속도 면에서는 CBOW가 더 뛰어남 skip-gram은 손실을 맥락의 수만큼 구해야 하기 때문

Q. skip-gram은 입력을 하나만 쓰는데 왜 정밀도 면에서 좋은건지
A: 여러 개의 입력이 아닌 하나만을 가지고 주변의 맥락을 추출하는 것이기에 더 고도화되는 것이 아닐지 (여러 의견이 있었음)

Q. skip-gram이 개별 손실을 모두 더하는데 이때 정해진 비율이 있는지
A: 따로 가중치를 두지 않고 전부 합침

Q. 네거티브 샘플링 보정에 0.75승을 사용하는데, 왜 굳이 0.75인지
A: 0.75가 아닌 다른 값을 사용해도 됨 "하이퍼파라미터"