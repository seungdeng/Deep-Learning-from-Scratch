Q1. 시그모이드(sigmoid)를 오래전부터 이용해왔으나, 최근에는 ReLU를  주로 이용한다는데 이유가 무엇인지
A. 시그모이드 함수의 한계점: 출력값 범위 0~1중 입력값이 매우 크거나 작을 경우 기울기가 매우 작아 학습 속도가 느려지고 역전파 과정에서 기울기 소실 문제 발생.
중간값(0.5) 근처에 대부분의 출력이 몰리는 현상이 있어 데이터 분포를 왜곡하여 학습 효율을 떨어뜨리고, 초기 가중치에 크게 의존하게 만듬.
시그모이드 함수의 출력은 항상 양수(0~1)이므로, 출력의 평균이 0이 아니며 가중치 업데이트시 양수/음수 균형이 맞지 않게 되어 역전파 최적화 과정의 효율을 떨어뜨림.

ReLU의 장점: 기울기가 1로 일정하게 유지되므로 기울기 소실 문제 해결.
ReLU는 비교 연산과 0으로의 변환만 수행하므로, 연산량이 적어 대규모 신경망에서 효과적임
희소성 - 입력값이 음수일 경우 출력이 0이므로, 일부 뉴런이 비활성화(출력 0)이 됨->뉴런간 독립성을 높이고 네트워크가 더 간결한 표현을 학습하도록 도움
기울기가 일정하여 출력의 중심화 문제가 발생하지 않음

Q2. 신경망 구현에서 손실함수로 교차 엔트로피를 주로 사용했는데, 오차제곱합을 사용하지 않은 이유가 무엇인지 
A. 분류 문제에서 더 적합한 성질을 가지기 때문.
교차 엔트로피: 확률 분포 간의 차이를 측정하는 지표. 예측값과 정답 원-핫 레이블 간의 차이를 최소화함
Softmax와 결합하여 사용할 때 효과적임.

오차 제곱합(MSE) 문제점: 연속적인 값의 차이를 측정하는 데 적합하므로 회귀 문제에서 주로 사용.
분류 문제는 출력값이 확률로 해석되기 때문에, 효과적으로 반영 불가.
예측 값을 정답 레이블과 가까운 중간 값으로 만드는 경향이 있음. 분류 문제에서 부정확한 결과 초래 가능.

Q3. 매개변수 갱신의 AdaGrad방법의 학습률 감소 기법은 조기에 학습이 거의 완료되면 자원낭비가 아닌지
A. 학습률 감소 문제를 해결하기 위해 RMSProp(기울기 제곱합 대신 지수 이동 평균 사용) 알고리즘, Adam, 학습률 스케쥴링 등의 대안이 있음.
대규모 데이터셋: RMSProp이나 Adam과 같은 대안 알고리즘이 적합.
빠른 초기 학습이 중요한 경우: AdaGrad를 사용하되, 학습률 스케줄링을 병행.
정밀한 최적화가 필요한 경우: Adam이나 RMSProp 사용.

Q4. 풀링(pooling)은 데이터 축소의 효과가 있지만 오차도 비례해서 커지고 정밀함이 없어지는 것이 아닌지
A. 윈도우 크기를 작게, 풀링 방식 선택(Max, Average, Global-전체 데이터 범위를 단일 값으로 요약. 주로 분류 문제의 마지막 계층에서 사용)
현대 CNN에서는 풀링 대신 Stride가 있는 Convolution을 사용해 차원을 줄이며 정보를 보존하려 시도함(ex: ResNet)
풀링 계층의 병렬 처리(ex: Max + Average)하여 정보 손실을 줄임.
손실된 정보를 보완하기 위해 Residual Connection(잔차 연결)을 활용.
데이터 증강 기법을 통해 데이터 축소로 인한 성능 저하 줄이기.

풀링 기법을 쓰기 위한 적합한 상황: 이미지의 중요한 특징만을 학습하면 충분한 경우(예: 객체 인식, 분류), 데이터 크기를 줄여 계산 효율성을 높이고, 과적합을 방지하려는 경우.

풀링 기법을 쓰기 적합하지 않은 상황: 세부적인 정보를 유지해야 하는 문제(예: 초해상도 이미지 복원, 경계선 검출), 데이터의 미세한 차이가 중요한 영향을 미치는 문제(예: 의료 영상 분석)

Q5. 합성곱에서 매개변수 수를 줄이기 위해 층을 깊게 구성하는데, 층을 깊게 하면 처리 속도가 더 느려지는 것이 아닌지
A. 층을 깊게하면 매개변수 감소, 표현력 증가, 정확도 향상의 효과가 있음.
하지만 계산량 증가, 병렬 처리(계층간 종속성으로 인해)의 어려움, 메모리 사용 증가,비효율적인 학습(너무 깊은 네트워크는 학습 속도가 느리고, 기울기 소실 문제의 가능성)

깊은 네트워크의 속도저하 완화 방법
1. 네트워크 설계 개선: 병렬 네트워크 구조(ex: GoogLeNet의 Inception모듈), 잔차 연결 
2. 필터 크기 최적화(작은 필터 여러번 사용하기)
3. 병렬 처리: GPU, TPU 사용
4. 점진적 차원 축소: 풀링, Stride가 있는 Convolution(풀링 계층 없이 합성곱 계층에서 Stride로 공간 차원을 줄여 계산량 감소.)
5. 경량 모델 사용: MobileNet, SqueezeNet은 깊이 유지하며 매개변수 수와 연산량을 줄이는 설계 기법을 채택함.
6. 지수적 감소 구조 사용: 초기에는 큰 크기 층 사용하고, 뒤로 갈수록 작은 크기의 층을 사용하여 계산량을 줄임.

네트워크 설계는 속도-정확도 트레이드오프를 고려해야 함. 속도와 정확도간의 균형을 맞추는 것이 중요함?
