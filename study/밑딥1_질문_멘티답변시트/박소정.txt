Q1. 기울기가 0인 장소를 찾지만 최솟값이 아닌 극솟값, 안장점일 가능성이 존재하는데 극솟값과 안장점이라는 단서를 발견할 수 있는 방법
A. 2차 미분(Hessian 행렬)을 활용하는 것이 일반적인 방법. 이를 통해 손실 함수의 곡률 정보를 확인하고, 해당 지점의 성질을 알 수 있다. 
또는, 딥러닝과 같은 고차원 문제에서는 모멘텀으로 보완하기도 한다.

Q2. SGD의 단점에도 불구하고 많은 연구에서 사용하는 이유 (매개변수 갱신의 장단점을 고려한 상황별 채택 방안)
A. 계산 효율성 - 한번의 업데이트에 하나의 샘플(미니배치)만 사용하므로 전체 데이터셋을 사용하는 경사 하강법보다 효율적
노이즈 기반 탐색 - 노이즈가 포함된 경사 방향으로 탐색하기 때문에, 지역 최적값이나 안장점을 벗어나 더 나은 전역 최적값에 도달할 가능성이 높음. 일부 연구에서는 SGD의 노이즈가 일반화 성능을 향상시킨다는 연구가 있음
SGD 단독 사용 경우 - 작은 데이터셋, 학습시간의 제한, 손실함수가 단순할 때

Q3. 가중치 감소 시 L2, L1의 특징과 이 중 L1과 비교했을 때, L2 노름이 일반적으로 더 사용되는지
A. L1 - 가중치를 절대값을 규제항으로 추가. 특징으로는 희소성(많은 가중치를 0으로 만듬, 일부 특징이 제거되면서 희소 모델이 만들어짐), 해석 용이성(희소성이 높아지므로 특징의 수를 줄임), 선택적 특징 학습(불필요한 특징을 효과적으로 제거, 고차원 데이터에서 유용함), 비선형 변화(L1 노름의 기울기는 절대값의 형태로 인해 매개변수의 변화에 일정하지 않음, 단 0근처에서는 일정)
L2 - 가중치의 제곱합을 규제항으로 추가. 특징으로는 부드러운 감소(많은 가중치를 작게 만들지만, 0으로 만들지는 않고 과적합 방지에 효과적), 일반화 성능(복잡도를 제어하여 오버피팅을 방지), 자연스러운 분산 감소
L2가 일반적으로 더 많이 사용되는 이유: L2노름은 기울기가 부드럽게 변하므로 최적화 과정이 안정적이고 수렴 속도가 빠름. L1은 절대값 함수의 특성상 기울기가 갑작스럽게 변할 수 있어 최적화가 불안정해질 가능성이 있음

결론 - L2 노름은 안정성과 과적합 방지 효과로 인해 일반적인 딥러닝 및 머신러닝 문제에서 더 많이 사용됨
L1 노름은 특징 선택이 중요한 문제(예: 희소 모델 생성)에서 사용되며, 딥러닝보다는 회귀 분석이나 고차원 데이터 처리에서 더 적합

Q4. 출력에 가까운 층 또는 마지막 출력 계층에서 Affine 조합이 사용되지 않는다면
A. Affine계층(선형 변환)은 출력 차원을 원하는 형태로 변환하는 역할을 함.
마지막 출력 계층에서 아핀조합이 없다면 -> 이전 은닉층의 출력 차원이 정해진 출력 형식을 제공하지 못할 가능성이 생김. 
선형 변환 없이 활성화 함수만 사용하면 함수의 값 범위에 제한이 생김: Softmax가 제대로 작동하지 않을 수 있음. 회귀문제에서 ReLU만 사용한다면 출력값이 음수가 될 수 없음
최적화 문제: 선형 변환이 없다면  출력값의 분포가 제한되므로 손실함수 값 왜곡의 가능성이 있음