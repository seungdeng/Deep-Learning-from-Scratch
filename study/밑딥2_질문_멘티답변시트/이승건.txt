Q1. 103p SVD 특잇값분해 과정에서, 원래 행렬 X에서 대각행렬 S로 특잇값을 추출하여 변형할 때, 어떠한 방식으로 계산하여 변형하는지?

Q2. chap4.word2vec 속도개선 156p 그림4-5에서 Embedding 배열의 원소 중 값(행 번호)이 같은 원소가 있다면, dW에 dh를 할당이 아닌 더하기를 해줘야 한다는데, 더하기를 하는 이유가? (찾아봐도 이해를 못하겠음)
A. 미분을 하게 되면 해당 매개변수 이외의 변수들은 상수 취급이므로 자연스럽게 소멸. 따라서 더하기로 이루어진 식은 각 인덱스마다 고유하게 독립적으로 계산이 가능하다.

*Q3. chap4.word2vec 속도개선 161p 네거티브 샘플링에서, 이진 분류 문제로 근사하기 위해 contexts 로부터 어떻게 하나의 target에 대한 이진 분류 문제로 바로 도달할 수 있는지? 은닉층 출력은 변함없이 실행하고, 점수(score) 최고값 하나만 Sigmoide를 통해 정규화하겠다는건지?

Q4. chap4.word2vec 속도개선 169p - 네거티브 샘플링에서, 계산량이 늘어나더라도 샘플을 늘릴수록 학습이 더 잘 진행되는 것이 아닌지? 

Q5. chap4.word2vec 속도개선 171p - 네거티브 샘플링의 샘플링 기법 중에, 각 단어의 빈도를 기준으로 확률분포로 나타내고 샘플링하는데, 이 방법이 제일 효과적인 샘플링 기법인지? 아니라면 어떠한 샘플링 기법이 있는지?

Q6. chap5. RNN 204p - Truncated BPTT에서, 역전파를 잘라내면 잘라진 경계면에선 이전 계층으로 역전파의 정보가 넘어가지 않는데 모든 시계열 데이터에 대해 가중치 갱신이 가능한건지?
A. 역전파의 계산은 구간별로 독립적으로 계산이 되고, 블록(구간 길이)마다 학습 결과가 달라질 수는 있으나 
순전파의 연결은 유지되기 때문에 앞 구간에서의 학습 정보가 어느정도 반영되므로 그 격차가 커지진 않는다. 

Q7. chap6. 게이트가 추가된 RNN 256p - 기억셀 c의 역전파 과정에서, 각 시각의 게이트 값이 다르다고 해서 기울기 소실이 안일어나는게 맞는지? forget 게이트는 (0~1)사이의 값인데(시그모이드 함수를 사용하기 때문) 소실되는건 마찬가지 아닌지?

Q8. chap6. 게이트가 추가된 RNN 273p - 왜 RNN이 일반적인 피드포워드 방식보다 오버피팅이 더 많이 발생하는지?
A. 모델이 복잡하고, 긴 시계열, 매개변수 공유로 인해 더 많은 학습을 진행하기 때문 

Q9. chap6. 게이트가 추가된 RNN 278p - LSTM 계선사항으로 Affine과 Embedding 계층의 가중치를 공유한다면, 개선이 아니라 오히려 overfitting으로 이어질수 있는 것이 아닌지?

*Q10. chap8. 어텐션 350p부근 - 어텐션계층에서도 학습이 이루어지는지?
A. 학습 매개변수가 따로 없어서 학습은 안되지만, 중요한 역할을 한다. 

Q11. chap8. 어텐션 356p - Affine계층에서 어텐션 계층의 출력과 그 시각의 LSTM계층의 출력값을 같이 입력받는 이유가 무엇인지

Q12. chap8. 어텐션 370p - 양방향 LSTM에서, 각 시각 LSTM의 출력(은닉 벡터 h)로 연결, 합, 평균 등의 방법이 있는데 어떤 방법이 좋은지
