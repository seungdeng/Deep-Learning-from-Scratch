Q1. 103p SVD 특잇값분해 과정에서, 원래 행렬 X에서 대각행렬 S로 특잇값을 추출하여 변형할 때, 
어떠한 방식으로 계산하여 변형하는지?

Q2. chap4.word2vec 속도개선 156p 그림4-5에서 Embedding 배열의 원소 중 값(행 번호)이 같은 원소가 있다면,
 dW에 dh를 할당이 아닌 더하기를 해줘야 한다는데, 더하기를 하는 이유가? (찾아봐도 이해를 못하겠음)
A. 미분을 하게 되면 해당 매개변수 이외의 변수들은 상수 취급이므로 자연스럽게 소멸. 따라서 더하기로 이루어진 식은 각 인덱스마다 고유하게 독립적으로 계산이 가능하다.

*Q3. chap4.word2vec 속도개선 161p 네거티브 샘플링에서, 이진 분류 문제로 근사하기 위해 
contexts 로부터 어떻게 하나의 target에 대한 이진 분류 문제로 바로 도달할 수 있는지? 은닉층 출력은 변함없이 실행하고, 
점수(score) 최고값 하나만 Sigmoide를 통해 정규화하겠다는건지?

Q4. chap4.word2vec 속도개선 169p 네거티브 샘플링에서, 계산량이 늘어나더라도 
샘플을 늘릴수록 학습이 더 잘 진행되는 것이 아닌지? 

Q5. chap4.word2vec 속도개선 171p 
네거티브 샘플링의 샘플링 기법 중에, 각 단어의 빈도를 기준으로 확률분포로 나타내고 샘플링하는데, 
이 방법이 제일 효과적인 샘플링 기법인지? 아니라면 어떠한 샘플링 기법이 있는지?

Q6. chap5. RNN 204p Truncated BPTT에서, 역전파를 잘라내면 잘라진 경계면에선 이전 계층으로 역전파의 정보가 넘어가지 않는데 
모든 시계열 데이터에 대해 가중치 갱신이 가능한건지?
A. 역전파의 계산은 구간별로 독립적으로 계산이 되고, 블록(구간 길이)마다 학습 결과가 달라질 수는 있으나 
순전파의 연결은 유지되기 때문에 앞 구간에서의 학습 정보가 어느정도 반영되므로 그 격차가 커지진 않는다. 
